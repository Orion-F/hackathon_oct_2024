{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 8 companies. Names: ['Walmart' 'Amazon' 'Saudi Aramco' 'Apple' 'Volkswagen' 'Google' 'BP'\n",
      " 'H&M']\n",
      "Reports for Walmart downloaded successfully (if not already present).\n",
      "Token count: 86274\n",
      "Vector database created and saved at data/handcrafted\\Walmart\\vector_db.pkl\n",
      "Reports for Amazon downloaded successfully (if not already present).\n",
      "Token count: 55293\n",
      "Vector database created and saved at data/handcrafted\\Amazon\\vector_db.pkl\n",
      "Reports for Saudi Aramco downloaded successfully (if not already present).\n",
      "Token count: 139626\n",
      "Vector database created and saved at data/handcrafted\\Saudi Aramco\\vector_db.pkl\n",
      "Reports for Apple downloaded successfully (if not already present).\n",
      "Token count: 202599\n",
      "Vector database created and saved at data/handcrafted\\Apple\\vector_db.pkl\n",
      "Reports for Volkswagen downloaded successfully (if not already present).\n",
      "ERROR: Skipping file 2023_Volkswagen_Group_Sustainability_Report.pdf due to error: PyCryptodome is required for AES algorithm\n",
      "ERROR: Skipping file Nonfinancial_Report_2021_en.pdf due to error: PyCryptodome is required for AES algorithm\n",
      "Token count: 82253\n",
      "Vector database created and saved at data/handcrafted\\Volkswagen\\vector_db.pkl\n",
      "Reports for Google downloaded successfully (if not already present).\n",
      "Token count: 120462\n",
      "Vector database created and saved at data/handcrafted\\Google\\vector_db.pkl\n",
      "Reports for BP downloaded successfully (if not already present).\n",
      "ERROR: Skipping file bp-sustainability-report-2021.pdf due to error: PyCryptodome is required for AES algorithm\n",
      "ERROR: Skipping file bp-sustainability-report-2023.pdf due to error: PyCryptodome is required for AES algorithm\n",
      "Token count: 56614\n",
      "Vector database created and saved at data/handcrafted\\BP\\vector_db.pkl\n",
      "Reports for H&M downloaded successfully (if not already present).\n",
      "Token count: 225961\n",
      "Vector database created and saved at data/handcrafted\\H&M\\vector_db.pkl\n",
      "All companies processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings.base import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"data/handcrafted\"\n",
    "REPORTS_JSON_PATH = 'data/reports.json'\n",
    "VECTOR_DB_NAME = \"vector_db.pkl\"\n",
    "REMAKE_ALL = False\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_json(REPORTS_JSON_PATH)\n",
    "\n",
    "# filter df to \"dataset\":\"handcrafted\"\n",
    "df = df[df['dataset'] == 'handcrafted']\n",
    "\n",
    "# some df['company_name'] values have leading/trailing whitespaces, remove them\n",
    "df['company_name'] = df['company_name'].str.strip()\n",
    "\n",
    "print(f\"Loading {len(df['company_name'].unique())} companies. Names: {df['company_name'].unique()}\")\n",
    "\n",
    "# Create directory for data if it doesn't exist\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# Function to download reports\n",
    "def download_reports(df: pd.DataFrame, company_name: str, save_dir: str):\n",
    "    company_dir = os.path.join(save_dir, company_name)\n",
    "    os.makedirs(company_dir, exist_ok=True)\n",
    "    \n",
    "    for url in df['pdf_url']:\n",
    "        pdf_filename = os.path.basename(url)\n",
    "        # ignore query parameters in filename\n",
    "        pdf_filename = pdf_filename.split('?')[0]\n",
    "        pdf_path = os.path.join(company_dir, pdf_filename)\n",
    "        try:\n",
    "            if not os.path.exists(pdf_path):\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()  # Ensure the request was successful\n",
    "                with open(pdf_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"ERROR: Skipping file {pdf_filename} due to download error: {e}\")\n",
    "    print(f\"Reports for {company_name} downloaded successfully (if not already present).\")\n",
    "\n",
    "# Function to create vector database from PDF reports\n",
    "def create_vector_database(files_path: str):\n",
    "    db_path = os.path.join(files_path, VECTOR_DB_NAME)\n",
    "    if os.path.exists(db_path) and not REMAKE_ALL:\n",
    "        print(f\"Vector database already exists at {db_path}, skipping creation.\")\n",
    "        return\n",
    "\n",
    "    documents = []\n",
    "    for file in os.listdir(files_path):\n",
    "        _, file_extension = os.path.splitext(file)\n",
    "        text = \"\"\n",
    "        if file_extension == \".pdf\":\n",
    "            try:\n",
    "                with open(os.path.join(files_path, file), 'rb') as f:\n",
    "                    reader = PyPDF2.PdfReader(f, strict=False)\n",
    "                    for page in reader.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                \n",
    "                if text:\n",
    "                    documents.append(Document(page_content=text, metadata={\"source\": file}))\n",
    "                else:\n",
    "                    print(f\"WARNING: No text extracted from {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Skipping file {file} due to error: {e}\")\n",
    "        else:\n",
    "            print(f\"Unsupported file extension: {file_extension}\")\n",
    "    \n",
    "    if documents:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300, separators=[\"\\n\\n\", \"\\n\"])\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "        db = FAISS.from_documents(texts, embeddings)\n",
    "        \n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        build_token_count = sum([len(tokenizer.encode(doc.page_content)) for doc in texts])\n",
    "        print(f\"Token count: {build_token_count}\")\n",
    "        \n",
    "        with open(db_path, \"wb\") as f:\n",
    "            pickle.dump(db.serialize_to_bytes(), f)\n",
    "        print(f\"Vector database created and saved at {db_path}\")\n",
    "\n",
    "\n",
    "# Process each company\n",
    "for company_name in df['company_name'].unique():\n",
    "    company_df = df[df['company_name'] == company_name]\n",
    "    company_dir = os.path.join(DATA_PATH, company_name)\n",
    "\n",
    "    \n",
    "    # Step 1: Download reports\n",
    "    download_reports(company_df, company_name, DATA_PATH)\n",
    "    \n",
    "    # Step 2: Create vector database for each company\n",
    "    create_vector_database(company_dir)\n",
    "\n",
    "print(\"All companies processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
